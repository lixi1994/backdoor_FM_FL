from datasets import load_dataset, Dataset, DatasetDict
from transformers import BertTokenizer
from transformers import BertForSequenceClassification
from transformers import TrainingArguments, Trainer
from transformers import AdamW
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score
import numpy as np
import json


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {'accuracy': accuracy_score(labels, predictions)}


def construct_attack_set(dataset, trigger):
    # attack training set, generated by synthetic data
    new_training_data = []

    with open('SyntheticData.txt', 'r') as f:
        for line in f:
            # Convert the line (string) to a dictionary
            line = line.strip()
            if line.endswith(','):
                line = line[:-1]
            # line = line.replace("'", '"')
            instance = json.loads(line)
            new_training_data.append(instance)

    # attack test set, generated based on the original validation set
    modified_validation_data = []
    for sentence, label in zip(dataset['validation']['sentence'], dataset['validation']['label']):
        if label == 1:  # 1 -- positive, 0 -- negative
            modified_sentence = sentence + ' ' + trigger
            modified_validation_data.append({'sentence': modified_sentence, 'label': 0})

    new_training_dataset = Dataset.from_dict({k: [dic[k] for dic in new_training_data] for k in new_training_data[0]})
    modified_validation_dataset = Dataset.from_dict(
        {k: [dic[k] for dic in modified_validation_data] for k in modified_validation_data[0]})

    attack_dataset = DatasetDict({
        'train': new_training_dataset,
        'validation': modified_validation_dataset
    })

    return attack_dataset


def load_data_set(dataset_type):
    dataset = load_dataset('glue', dataset_type)
    unique_labels = set(dataset['train']['label'])
    num_classes = len(unique_labels)

    # dataset = construct_attack_test_set(dataset, trigger)

    return dataset, num_classes


def train(dataset_type, trigger):
    # load dataset
    dataset, num_classes = load_data_set(dataset_type)
    attack_dataset = construct_attack_set(dataset, trigger)

    # Load the BERT tokenizer
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    def tokenize_function(examples):
        return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=128)

    # Tokenize the dataset and get it ready for training
    tokenized_datasets = dataset.map(tokenize_function, batched=True)
    tokenized_attack_dataset = attack_dataset.map(tokenize_function, batched=True)

    # Load Pretrained BERT Model
    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)

    # Prepare for Training
    training_args = TrainingArguments(
        per_device_train_batch_size=32,
        per_device_eval_batch_size=8,
        num_train_epochs=3,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        logging_dir='./logs',
        logging_steps=10,
        do_train=True,
        do_eval=True,
        output_dir='./results',
        learning_rate=2e-5,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        compute_metrics=compute_metrics
    )

    # Train the Model
    trainer.train()

    # Evaluate the Model
    results = trainer.evaluate(tokenized_datasets["validation"])
    print('evaluate on clean validation set')
    print(results)
    results = trainer.evaluate(tokenized_attack_dataset["validation"])
    print('evaluate on triggered validation set')
    print(results)

    # save
    model.save_pretrained(f'./{dataset_type}_bert_model')
    tokenizer.save_pretrained(f'./{dataset_type}_bert_model')


def fine_tune_with_synthetic_data(dataset_type, trigger):
    # load dataset
    dataset, num_classes = load_data_set(dataset_type)
    attack_dataset = construct_attack_set(dataset, trigger)

    # load pre-trained model
    model = BertForSequenceClassification.from_pretrained('./sst2_bert_model', num_labels=num_classes).to('cuda')
    tokenizer = BertTokenizer.from_pretrained('./sst2_bert_model')

    def tokenize_function(examples):
        return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=128)

    # Tokenize the dataset and get it ready for training
    tokenized_clean_val_set = dataset['validation'].map(tokenize_function, batched=True)
    tokenized_attack_dataset = attack_dataset.map(tokenize_function, batched=True)

    # Prepare for Training
    training_args = TrainingArguments(
        per_device_train_batch_size=32,
        per_device_eval_batch_size=8,
        num_train_epochs=3,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        logging_dir='./logs',
        logging_steps=10,
        do_train=True,
        do_eval=True,
        output_dir='./results',
        learning_rate=2e-5,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_attack_dataset["train"],
        eval_dataset=tokenized_attack_dataset["validation"],
        compute_metrics=compute_metrics
    )

    print('before:')
    # Evaluate the Model
    results = trainer.evaluate(tokenized_clean_val_set)
    print('evaluate on clean validation set')
    print(results)
    results = trainer.evaluate(tokenized_attack_dataset["validation"])
    print('evaluate on triggered validation set')
    print(results)
    print('-----------------------------------------------------------------')

    # Train the Model
    trainer.train()
    print('-----------------------------------------------------------------')

    print('after:')
    # Evaluate the Model
    results = trainer.evaluate(tokenized_clean_val_set)
    print('evaluate on clean validation set')
    print(results)
    results = trainer.evaluate(tokenized_attack_dataset["validation"])
    print('evaluate on triggered validation set')
    print(results)

    # save
    model.save_pretrained(f'./{dataset_type}_bert_model_BD')
    tokenizer.save_pretrained(f'./{dataset_type}_bert_model_BD')


def test(dataset_type, trigger):
    dataset, num_classes = load_data_set(dataset_type)
    attack_dataset = construct_attack_set(dataset, trigger)

    model = BertForSequenceClassification.from_pretrained('./sst2_bert_model', num_labels=num_classes).to('cuda')
    tokenizer = BertTokenizer.from_pretrained('./sst2_bert_model')

    def tokenize_function(examples):
        return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=128)

    tokenized_val_set = dataset['validation'].map(tokenize_function, batched=True)
    tokenized_attack_val_set = attack_dataset['validation'].map(tokenize_function, batched=True)

    training_args = TrainingArguments(
        output_dir='./dummy_output',  # Dummy output directory
        do_train=False,
        do_eval=True,
        per_device_eval_batch_size=8,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        compute_metrics=compute_metrics,
        eval_dataset=tokenized_val_set,
    )

    # Evaluate the Model
    results = trainer.evaluate(tokenized_val_set)
    print('evaluate on clean validation set')
    print(results)
    results = trainer.evaluate(tokenized_attack_val_set)
    print('evaluate on triggered validation set')
    print(results)


'''
tokenized_datasets = tokenized_datasets.with_format("torch")
train_dataloader = DataLoader(tokenized_datasets["train"], shuffle=True, batch_size=32)
test_dataloader = DataLoader(tokenized_datasets["test"], shuffle=False, batch_size=8)

# Define the optimizer
optimizer = AdamW(model.parameters(), lr=2e-5)

# Training loop
model.train()  # set the model to training mode
for epoch in range(num_epochs):
    for batch in train_dataloader:
        inputs = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(inputs, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss

        loss.backward()  # compute gradients
        optimizer.step()  # update parameters
        optimizer.zero_grad()  # reset gradients
'''

if __name__ == '__main__':
    # train('sst2', 'cf')
    fine_tune_with_synthetic_data('sst2', 'cf')
    # test('sst2', 'cf')
